{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "from datasets import Dataset \n",
    "\n",
    "import wandb \n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, TrainerCallback, AutoTokenizer, DataCollatorForSeq2Seq \n",
    "\n",
    "from peft import LoftQConfig, LoraConfig, get_peft_model \n",
    "\n",
    "\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "  \n",
    "\n",
    "# Initialize W&B \n",
    "\n",
    "wandb.init(project=\"language_model_finetuning\", entity=\"your_username\") \n",
    "\n",
    " \n",
    "\n",
    "# Custom callback for logging \n",
    "\n",
    "class LoggingCallback(TrainerCallback): \n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs): \n",
    "\n",
    "        if logs is not None: \n",
    "\n",
    "            wandb.log(logs) \n",
    "\n",
    "  \n",
    "\n",
    "# Initialize Weights & Biases \n",
    "\n",
    "wandb.init(project=\"t5-finetuning-peft\") \n",
    "\n",
    "  \n",
    "\n",
    "# Sample data \n",
    "\n",
    "#you can replace this sample data with your own data \n",
    "\n",
    "train_data = [ \n",
    "\n",
    "    {\"article\": \"Fine-tuning language models can significantly improve their performance on specialized tasks. By training the model on task-specific data, it can learn the nuances of the task and provide better results.\", \"summary\": \"Fine-tuning improves model performance on specialized tasks.\"}, \n",
    "\n",
    "    {\"article\": \"Quantized methods like QLoRA are becoming popular for efficient fine-tuning of large language models. These methods reduce the computational resources required for training while maintaining accuracy.\", \"summary\": \"QLoRA enables efficient fine-tuning of large models.\"}, \n",
    "\n",
    "    {\"article\": \"Weight and Biases (W&B) is a powerful tool for tracking and visualizing machine learning experiments. It helps data scientists monitor the training process and gain insights into model performance.\", \"summary\": \"W&B aids in tracking and visualizing ML experiments.\"}, \n",
    "\n",
    "    {\"article\": \"Using pre-trained models as a starting point for fine-tuning can save time and resources. Pre-trained models have already learned a vast amount of information from large datasets.\", \"summary\": \"Pre-trained models save time and resources.\"}, \n",
    "\n",
    "    {\"article\": \"During fine-tuning, it is essential to monitor the model's loss and metrics to ensure it is learning correctly. Visualization tools can help in understanding the training process.\", \"summary\": \"Monitoring loss and metrics is crucial during fine-tuning.\"}, \n",
    "\n",
    "    {\"article\": \"Data augmentation techniques can help improve the robustness of machine learning models. By artificially increasing the diversity of the training data, models can generalize better to new, unseen data.\", \"summary\": \"Data augmentation improves model robustness.\"}, \n",
    "\n",
    "    {\"article\": \"Transfer learning involves using a pre-trained model on a new, but related task. This approach can accelerate the training process and improve model performance.\", \"summary\": \"Transfer learning accelerates training and improves performance.\"}, \n",
    "\n",
    "    {\"article\": \"Hyperparameter tuning is a critical step in optimizing model performance. Techniques like grid search and random search are commonly used to find the best hyperparameter values.\", \"summary\": \"Hyperparameter tuning optimizes model performance.\"}, \n",
    "\n",
    "    {\"article\": \"Regularization methods like dropout and weight decay help prevent overfitting in neural networks. These techniques ensure that the model generalizes well to new data.\", \"summary\": \"Regularization prevents overfitting in neural networks.\"}, \n",
    "\n",
    "    {\"article\": \"Cross-validation is a robust method for assessing the performance of machine learning models. It provides a more accurate estimate of model performance than a single train-test split.\", \"summary\": \"Cross-validation provides accurate performance estimates.\"} \n",
    "\n",
    "]  \n",
    "\n",
    "test_data = [ \n",
    "\n",
    "    {\"article\": \"Active learning can reduce the amount of labeled data needed for training. By selectively choosing the most informative samples, models can learn more efficiently.\", \"summary\": \"Active learning reduces the need for labeled data.\"}, \n",
    "\n",
    "    {\"article\": \"Ensemble methods combine the predictions of multiple models to improve accuracy. Techniques like bagging and boosting are popular ensemble methods.\", \"summary\": \"Ensemble methods improve prediction accuracy.\"}, \n",
    "\n",
    "    {\"article\": \"Gradient descent is a fundamental optimization algorithm used to train machine learning models. Variants like stochastic gradient descent (SGD) offer improvements in convergence speed.\", \"summary\": \"Gradient descent and its variants optimize model training.\"}]  \n",
    "\n",
    "evaluation_data = [ \n",
    "\n",
    "    {\"article\": \"Feature engineering is the process of using domain knowledge to create features that make machine learning algorithms work better. It is a crucial step in building effective models.\", \"summary\": \"Feature engineering enhances model effectiveness.\"}, \n",
    "\n",
    "    {\"article\": \"Batch normalization is a technique used to improve the training of deep neural networks. It normalizes the input of each layer, allowing for faster and more stable training.\", \"summary\": \"Batch normalization stabilizes and speeds up training.\"}, \n",
    "\n",
    "    {\"article\": \"Model interpretability is important for understanding how machine learning models make decisions. Techniques like SHAP and LIME provide insights into model predictions.\", \"summary\": \"Model interpretability techniques provide insights into predictions.\"}]  \n",
    "\n",
    "# Convert data to Hugging Face datasets \n",
    "\n",
    "train_dataset = Dataset.from_list(train_data) \n",
    "\n",
    "test_dataset = Dataset.from_list(test_data) \n",
    "\n",
    "eval_dataset = Dataset.from_list(evaluation_data) \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# Load the T5 tokenizer and model \n",
    "\n",
    "# = 't5-small' tokenizer model have 60 million trainable parameter and storage size is 987mb \n",
    "\n",
    " \n",
    "\n",
    "model_name = 't5-small' \n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name) \n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(model_name) \n",
    "\n",
    "  \n",
    "\n",
    "# Apply PEFT using LoRA \n",
    "\n",
    "loftq_config = LoftQConfig(loftq_bits=4)           # set 4bit quantization \n",
    "\n",
    "lora_config = LoraConfig( \n",
    "\n",
    "    r=6,  # Rank of the adaptation matrices \n",
    "\n",
    "    lora_alpha=16,  # When the weight changes are added back into the original #model weights, they are multiplied by a scaling factor that's #calculated as alpha divided by rank. \n",
    "\n",
    "    loftq_config=loftq_config, \n",
    "\n",
    "    target_modules=[\"k\",\"q\", \"v\"],  # Target modules for LoRA adaptation \n",
    "\n",
    "    lora_dropout=0.1,  # Dropout rate \n",
    "\n",
    "    bias=\"none\"  # Whether to adapt bias terms \n",
    "\n",
    ") \n",
    "\n",
    "  \n",
    "\n",
    "peft_model = get_peft_model(base_model, lora_config) \n",
    "\n",
    "peft_model.print_trainable_parameters() \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Preprocessing the datasets \n",
    "\n",
    "def preprocess_function(examples): \n",
    "\n",
    "    inputs = tokenizer(examples[\"article\"], padding=\"max_length\", truncation=True, max_length=512) \n",
    "\n",
    "    targets = tokenizer(examples[\"summary\"], padding=\"max_length\", truncation=True, max_length=128) \n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"] \n",
    "\n",
    "    return inputs \n",
    "\n",
    "  \n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True) \n",
    "\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True) \n",
    "\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True) \n",
    "\n",
    " # Define the compute_metrics function \n",
    "\n",
    "def compute_metrics(pred: EvalPrediction): \n",
    "\n",
    "    metric = load_metric(\"rouge\") \n",
    "\n",
    "    labels_ids = pred.label_ids \n",
    "\n",
    "    pred_ids = pred.predictions \n",
    "\n",
    "    # Decode the predicted and true labels \n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True) \n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels_ids, skip_special_tokens=True) \n",
    "\n",
    "    # Rouge expects a newline after each sentence \n",
    "\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds] \n",
    "\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels] \n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels) \n",
    "\n",
    "    return { \n",
    "\n",
    "    \"rouge1\": result[\"rouge1\"].mid.fmeasure \n",
    "\n",
    "    } \n",
    "\n",
    " \n",
    "\n",
    "# Define training arguments \n",
    "\n",
    "training_args = TrainingArguments(do_eval=True, \n",
    "\n",
    "    output_dir=\"./results\", \n",
    "\n",
    "    eval_strategy=\"steps\",  # Change to \"steps\" for more frequent logging \n",
    "\n",
    "    eval_steps=10,  # Evaluate every 10 steps \n",
    "\n",
    "    logging_dir='./logs',  # Directory for storing logs \n",
    "\n",
    "    logging_steps=20,  # Log every 5 steps \n",
    "\n",
    "    learning_rate=2e-5, \n",
    "\n",
    "    per_device_train_batch_size=20, \n",
    "\n",
    "    per_device_eval_batch_size=20, \n",
    "\n",
    "    num_train_epochs=30, \n",
    "\n",
    "    weight_decay=0.01, \n",
    "\n",
    "    report_to=\"wandb\", \n",
    "\n",
    ") \n",
    "\n",
    "  \n",
    "\n",
    "# Initialize the Trainer \n",
    "\n",
    "trainer = Trainer( \n",
    "\n",
    "    model=peft_model, \n",
    "\n",
    "    args=training_args, \n",
    "\n",
    "    train_dataset=train_dataset, \n",
    "\n",
    "    eval_dataset=eval_dataset, \n",
    "\n",
    "    tokenizer=tokenizer, \n",
    "\n",
    "compute_metrics=compute_metrics, \n",
    "\n",
    "    callbacks=[LoggingCallback()]  # Add the custom logging callback \n",
    "\n",
    ") \n",
    "\n",
    "  \n",
    "\n",
    "# Fine-tune the model \n",
    "\n",
    "trainer.train() \n",
    "\n",
    "eval_results = trainer.evaluate() \n",
    "\n",
    "  \n",
    "\n",
    "# Save the model \n",
    "\n",
    "trainer.save_model(\"./t5-finetuned\") \n",
    "\n",
    "\n",
    " # Load LoRA adapter and merge \n",
    "\n",
    "merged_model = peft_model.merge_and_unload() \n",
    "\n",
    "# Save the merged model \n",
    "\n",
    "merged_model.save_pretrained(\"./t5-finetuned\") \n",
    "\n",
    " \n",
    "# Finish the W&B run \n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
